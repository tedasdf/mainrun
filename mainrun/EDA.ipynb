{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a35e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "num_titles = 10000\n",
    "val_frac = 0.1\n",
    "seed = 1337\n",
    "ds = load_dataset(\"julien040/hacker-news-posts\", split=\"train\", cache_dir=\"./data\").shuffle(seed=seed)\n",
    "titles = [row[\"title\"].strip() for row in ds.take(num_titles)]\n",
    "n = int(num_titles * (1 - val_frac))\n",
    "train_titles, val_titles= titles[:n], titles[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77977d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TITLES\n",
      "Doom on Ubuntu Phone\n",
      "VAL TITles\n",
      "New Twists in the Road to Quantum Supremacy\n"
     ]
    }
   ],
   "source": [
    "print('Train TITLES')\n",
    "print(train_titles[0])\n",
    "print('VAL TITles')\n",
    "print(val_titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4c0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, decoders, models, pre_tokenizers, trainers    \n",
    "\n",
    "\n",
    "def train_tokenizer(titles: list[str], vocab_size: int, unk_token: str = \"<unk>\", pad_token: str = \"<pad>\", eos_token: str = \"<eos>\") -> Tokenizer:\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=unk_token))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[pad_token, eos_token, unk_token]\n",
    "    )\n",
    "    tokenizer.train_from_iterator(titles, trainer)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(split_ids: torch.Tensor, ptr: int, block_size: int, batch_size: int, device: torch.device):\n",
    "    span = block_size * batch_size + 1\n",
    "    if ptr + span >= len(split_ids):\n",
    "        ptr = 0\n",
    "    batch = split_ids[ptr: ptr + span]\n",
    "    x = batch[:-1].view(batch_size, block_size).to(device)\n",
    "    y = batch[1:].view(batch_size, block_size).to(device)\n",
    "    return x, y, ptr + block_size * batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a5b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b756dc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"<pad>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":1, \"content\":\"<eos>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":2, \"content\":\"<unk>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=None, pre_tokenizer=ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True), post_processor=None, decoder=ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True), model=BPE(dropout=None, unk_token=\"<unk>\", continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"<pad>\":0, \"<eos>\":1, \"<unk>\":2, \"!\":3, \"\"\":4, \"#\":5, \"$\":6, \"%\":7, \"&\":8, \"'\":9, \"(\":10, \")\":11, \"*\":12, \"+\":13, \",\":14, \"-\":15, \".\":16, \"/\":17, \"0\":18, \"1\":19, \"2\":20, \"3\":21, \"4\":22, \"5\":23, \"6\":24, \"7\":25, \"8\":26, \"9\":27, \":\":28, \";\":29, \"<\":30, \"=\":31, \"?\":32, \"@\":33, \"A\":34, \"B\":35, \"C\":36, \"D\":37, \"E\":38, \"F\":39, \"G\":40, \"H\":41, \"I\":42, \"J\":43, \"K\":44, \"L\":45, \"M\":46, \"N\":47, \"O\":48, \"P\":49, \"Q\":50, \"R\":51, \"S\":52, \"T\":53, \"U\":54, \"V\":55, \"W\":56, \"X\":57, \"Y\":58, \"Z\":59, \"[\":60, \"]\":61, \"_\":62, \"`\":63, \"a\":64, \"b\":65, \"c\":66, \"d\":67, \"e\":68, \"f\":69, \"g\":70, \"h\":71, \"i\":72, \"j\":73, \"k\":74, \"l\":75, \"m\":76, \"n\":77, \"o\":78, \"p\":79, \"q\":80, \"r\":81, \"s\":82, \"t\":83, \"u\":84, \"v\":85, \"w\":86, \"x\":87, \"y\":88, \"z\":89, \"|\":90, \"~\":91, \"¡\":92, \"¢\":93, \"£\":94, \"¤\":95, \"¥\":96, \"¦\":97, \"§\":98, ...}, merges=[(\"i\", \"n\"), (\"e\", \"r\"), (\"o\", \"n\"), (\"Ġ\", \"t\"), (\"e\", \"s\"), (\"a\", \"n\"), (\"o\", \"r\"), (\"Ġ\", \"S\"), (\"a\", \"t\"), (\"Ġ\", \"A\"), (\"in\", \"g\"), (\"a\", \"r\"), (\"e\", \"n\"), (\"i\", \"t\"), (\"Ġ\", \"T\"), (\"h\", \"e\"), (\"Ġ\", \"C\"), (\"a\", \"l\"), (\"o\", \"u\"), (\"Ġ\", \"f\"), (\"s\", \"t\"), (\"r\", \"e\"), (\"l\", \"e\"), (\"Ġ\", \"H\"), (\"Ġ\", \"a\"), (\"Ġ\", \"P\"), (\"Ġ\", \"M\"), (\"r\", \"o\"), (\"i\", \"c\"), (\"Ġ\", \"W\"), (\"Ġt\", \"o\"), (\"e\", \"d\"), (\"Ġ\", \"I\"), (\"Ġ\", \"c\"), (\"Ġ\", \"s\"), (\"Ġ\", \"B\"), (\"Ġ\", \"D\"), (\"Ġ\", \"o\"), (\"Ġ\", \"w\"), (\"a\", \"c\"), (\"i\", \"on\"), (\"i\", \"s\"), (\"o\", \"w\"), (\"â\", \"Ģ\"), (\"Ġ\", \"R\"), (\"Ġ\", \"in\"), (\"Ġt\", \"he\"), (\"Ġ\", \"F\"), (\"Ġ\", \"b\"), (\"an\", \"d\"), (\"e\", \"t\"), (\"Ġ\", \"p\"), (\"o\", \"m\"), (\"Ġo\", \"f\"), (\"Ġ\", \"G\"), (\"i\", \"l\"), (\"Ġ\", \"d\"), (\"Ġ\", \"L\"), (\"e\", \"c\"), (\"a\", \"s\"), (\"Ġ\", \"E\"), (\"Ġ\", \"m\"), (\"Ġf\", \"or\"), (\"e\", \"l\"), (\"en\", \"t\"), (\"Ġ\", \"N\"), (\"Ġ\", \"and\"), (\"o\", \"o\"), (\"i\", \"d\"), (\"a\", \"m\"), (\"er\", \"s\"), (\"Ġ\", \"O\"), (\"i\", \"g\"), (\"ĠT\", \"he\"), (\"u\", \"t\"), (\"i\", \"v\"), (\"o\", \"l\"), (\"Ġ\", \"U\"), (\"ou\", \"r\"), (\"a\", \"d\"), (\"u\", \"s\"), (\"it\", \"h\"), (\"Ġ\", \"âĢ\"), (\"a\", \"y\"), (\"o\", \"t\"), (\"t\", \"er\"), (\"u\", \"r\"), (\"Ġ\", \"l\"), (\"ĠH\", \"N\"), (\"c\", \"h\"), (\"p\", \"p\"), (\"ĠS\", \"t\"), (\"ĠI\", \"n\"), (\"e\", \"w\"), (\"Ġ\", \"on\"), (\"Ġ\", \"h\"), (\"i\", \"m\"), (\"at\", \"ion\"), (\"Ġ\", \"y\"), ...]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenizer(train_titles+val_titles, vocab_size, eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79d80728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.tokenizer.BPETokenizer import BPETokenizer\n",
    "tok = BPETokenizer(train_tokenizer(train_titles+val_titles, vocab_size, eos_token=\"<eos>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9056ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token=\"<eos>\"\n",
    "train_text = eos_token.join(train_titles) + eos_token\n",
    "val_text = eos_token.join(val_titles) + eos_token\n",
    "train_ids = torch.tensor(tok.encode(train_text), dtype=torch.long)\n",
    "val_ids = torch.tensor(tok.encode(val_text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b176b84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_titles) , len(val_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04950b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([107337]), torch.Size([11963]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids.shape, val_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ba10127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6130,   274,  1866,  1723,     1,  5115,  5482,   246,  1228, 10936])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb4d04ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> Max length: 98, Index: 6099, Title: Rough silicon nanowires potentially allow much more efficient waste-heat to electricity conversion\n",
      "Validation -> Max length: 81, Index: 229, Title: Official Google Blog: \"This site may harm your computer\" on every search result??\n"
     ]
    }
   ],
   "source": [
    "# For train titles\n",
    "max_train_idx = max(range(len(train_titles)), key=lambda i: len(train_titles[i]))\n",
    "max_len_train = len(train_titles[max_train_idx])\n",
    "max_train_title = train_titles[max_train_idx]\n",
    "\n",
    "# For validation/test titles\n",
    "max_val_idx = max(range(len(val_titles)), key=lambda i: len(val_titles[i]))\n",
    "max_len_val = len(val_titles[max_val_idx])\n",
    "max_val_title = val_titles[max_val_idx]\n",
    "\n",
    "print(f\"Train -> Max length: {max_len_train}, Index: {max_train_idx}, Title: {max_train_title}\")\n",
    "print(f\"Validation -> Max length: {max_len_val}, Index: {max_val_idx}, Title: {max_val_title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9119ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.tokenizer.BPETokenizer import BPETokenizer\n",
    "from train import train_tokenizer\n",
    "\n",
    "\n",
    "eos_token = \"<eos>\"\n",
    "tok = BPETokenizer(train_tokenizer(train_titles+val_titles, 16000, eos_token=eos_token))\n",
    "train_text = eos_token.join(train_titles) + eos_token\n",
    "val_text = eos_token.join(val_titles) + eos_token\n",
    "train_ids = torch.tensor(tok.encode(train_text), dtype=torch.long)\n",
    "val_ids = torch.tensor(tok.encode(val_text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46ed4721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens in train: 68 Title: Unearthing Z͌̈́̾a͊̈́l͊̿g̏̉͆o̾̚̚S̝̬ͅc̬r̯̼͇ͅi̼͖̜̭͔p̲̘̘̹͖t̠͖̟̹͓͇ͅ with visual fuzzing\n",
      "Max tokens in val: 44 Title: Bangladeshi model Farhana Akhtar Nisho (ফারহানা আখতার নিশ্) hot and sexy photo\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each title individually\n",
    "train_token_lists = [tok.encode(title) for title in train_titles]\n",
    "val_token_lists = [tok.encode(title) for title in val_titles]\n",
    "\n",
    "# Find maximum token length and the corresponding title\n",
    "max_len_train = max(len(t) for t in train_token_lists)\n",
    "max_len_val = max(len(t) for t in val_token_lists)\n",
    "\n",
    "# Optional: get the index or title itself\n",
    "max_train_idx = max(range(len(train_token_lists)), key=lambda i: len(train_token_lists[i]))\n",
    "max_val_idx = max(range(len(val_token_lists)), key=lambda i: len(val_token_lists[i]))\n",
    "\n",
    "print(\"Max tokens in train:\", max_len_train, \"Title:\", train_titles[max_train_idx])\n",
    "print(\"Max tokens in val:\", max_len_val, \"Title:\", val_titles[max_val_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1917de2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107337, 11963)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ids) , len(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2a60e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in train: 25516\n",
      "Number of unique words in val: 4177\n"
     ]
    }
   ],
   "source": [
    "# Split text into words (assuming words are separated by spaces)\n",
    "train_words = train_text.split()\n",
    "val_words = val_text.split()\n",
    "\n",
    "# Find unique words using set\n",
    "unique_train_words = set(train_words)\n",
    "unique_val_words = set(val_words)\n",
    "\n",
    "print(\"Number of unique words in train:\", len(unique_train_words))\n",
    "print(\"Number of unique words in val:\", len(unique_val_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16184996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words in train + val: 27707\n"
     ]
    }
   ],
   "source": [
    "# Split text into words\n",
    "train_words = set(train_text.split())\n",
    "val_words = set(val_text.split())\n",
    "\n",
    "# Combine both sets to get all unique words\n",
    "all_unique_words = train_words | val_words  # union of sets\n",
    "\n",
    "print(\"Total number of unique words in train + val:\", len(all_unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f166a32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29693"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25516 + 4177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82e28b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in train: 14403\n",
      "Unique tokens in val: 4669\n",
      "Combined unique tokens: 14751\n",
      "Tokens in val not seen in train: 348\n"
     ]
    }
   ],
   "source": [
    "# Tokenize train and val text individually\n",
    "train_tokens = tok.encode(train_text)\n",
    "val_tokens = tok.encode(val_text)\n",
    "\n",
    "# Convert to sets to get unique tokens\n",
    "unique_train_tokens = set(train_tokens)\n",
    "unique_val_tokens = set(val_tokens)\n",
    "\n",
    "# Combined unique tokens across train and val\n",
    "combined_unique_tokens = unique_train_tokens | unique_val_tokens\n",
    "\n",
    "print(\"Unique tokens in train:\", len(unique_train_tokens))\n",
    "print(\"Unique tokens in val:\", len(unique_val_tokens))\n",
    "print(\"Combined unique tokens:\", len(combined_unique_tokens))\n",
    "\n",
    "# Find tokens in val but not in train (new tokens)\n",
    "tokens_only_in_val = unique_val_tokens - unique_train_tokens\n",
    "print(\"Tokens in val not seen in train:\", len(tokens_only_in_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "021a56d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15099"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14751+348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63d7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

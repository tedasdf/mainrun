hyperparameters_configured: model_arhitecture=gpt, block_size=128, batch_size=64, vocab_size=16000, n_layer=6, n_head=8, d_model=512, dropout=0.1, lr=0.006, weight_decay=0.0, evals_per_epoch=3, epochs=7, seed=1337, num_titles=100000, val_frac=0.1, log_file=./logs/mainrun.log
device_info: device=cpu
Traceback (most recent call last):
  File "/workspace/mainrun/train.py", line 256, in <module>
    main(cfg)
  File "/workspace/mainrun/train.py", line 132, in main
    tok = BPETokenizer(train_tokenizer(train_titles+val_titles, args.vocab_size, eos_token=eos_token))
  File "/workspace/mainrun/train.py", line 98, in train_tokenizer
    tokenizer.train_from_iterator(titles, trainer)
KeyboardInterrupt

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6baf86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd1ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "num_titles = 10000\n",
    "val_frac = 0.1\n",
    "seed = 1337\n",
    "ds = load_dataset(\"julien040/hacker-news-posts\", split=\"train\", cache_dir=\"./data\").shuffle(seed=seed)\n",
    "titles = [row[\"title\"].strip() for row in ds.take(num_titles)]\n",
    "n = int(num_titles * (1 - val_frac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46925a49",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BottleneckAttnConfig' from 'model.attention.attention' (c:\\Users\\teeds\\Desktop\\projects\\mainrun\\mainrun\\model\\attention\\attention.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m     optimizer: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m     21\u001b[0m     evals_per_epoch: \u001b[38;5;28mfloat\u001b[39m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPT, GPTConfig\n\u001b[0;32m     26\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mAttnConfig\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     d_model: \u001b[38;5;28mint\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\teeds\\Desktop\\projects\\mainrun\\mainrun\\model\\gpt.py:2\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     AttnConfig, \n\u001b[0;32m      4\u001b[0m     CausalSelfAttention,\n\u001b[0;32m      5\u001b[0m     SparseAttnConfig,\n\u001b[0;32m      6\u001b[0m     SparseCausalSelfAttention,\n\u001b[0;32m      7\u001b[0m     BottleneckAttnConfig,\n\u001b[0;32m      8\u001b[0m     CausalBottleneckAttn\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BottleneckAttnConfig' from 'model.attention.attention' (c:\\Users\\teeds\\Desktop\\projects\\mainrun\\mainrun\\model\\attention\\attention.py)"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "\n",
    "    seed: int\n",
    "    epochs: int\n",
    "    val_frac: float\n",
    "    num_titles: int\n",
    "    vocab_size: int\n",
    "    context_length: int  # Added context_length parameter\n",
    "\n",
    "    log_file: str\n",
    "    model_architecture: str \n",
    "    \n",
    "    batch_size: int\n",
    "    lr: float\n",
    "    weight_decay: float\n",
    "    scheduler: str # none, linear, cosine\n",
    "    optimizer: str\n",
    "    evals_per_epoch: float\n",
    "\n",
    "\n",
    "from model.gpt import GPT, GPTConfig\n",
    "\n",
    "@dataclass\n",
    "class AttnConfig:\n",
    "    d_model: int\n",
    "    n_head: int\n",
    "    block_size: int\n",
    "    dropout: float\n",
    "\n",
    "cfg = OmegaConf.load(\"config/hyperparams.yaml\")\n",
    "            # Update cfg with args\n",
    "\n",
    "hparams = OmegaConf.to_container(cfg.hyperparams, resolve=True)\n",
    "modelparams = OmegaConf.to_container(cfg.model_configs[hparams['model_architecture']], resolve=True)\n",
    "attnparams = OmegaConf.to_container(cfg.attn_configs[modelparams['attention_layer']], resolve=True)\n",
    "\n",
    "args = Hyperparameters(**hparams)\n",
    "\n",
    "attn = AttnConfig(\n",
    "    d_model=modelparams['d_model'],\n",
    "    n_head=attnparams['n_head'],\n",
    "    block_size=args.context_length,\n",
    "    dropout=modelparams['dropout']\n",
    ")\n",
    "\n",
    "cfg = GPTConfig(\n",
    "    vocab_size=args.vocab_size,\n",
    "    block_size=args.context_length,\n",
    "    attn_config = attn,\n",
    "    activation_function = 'gelu',\n",
    "    **modelparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f943360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6baf86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd1ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "num_titles = 10000\n",
    "val_frac = 0.1\n",
    "seed = 1337\n",
    "ds = load_dataset(\"julien040/hacker-news-posts\", split=\"train\", cache_dir=\"./data\").shuffle(seed=seed)\n",
    "titles = [row[\"title\"].strip() for row in ds.take(num_titles)]\n",
    "n = int(num_titles * (1 - val_frac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46925a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "\n",
    "    seed: int\n",
    "    epochs: int\n",
    "    val_frac: float\n",
    "    num_titles: int\n",
    "    vocab_size: int\n",
    "    context_length: int  # Added context_length parameter\n",
    "\n",
    "    log_file: str\n",
    "    model_architecture: str \n",
    "    \n",
    "    batch_size: int\n",
    "    lr: float\n",
    "    weight_decay: float\n",
    "    scheduler: str # none, linear, cosine\n",
    "    optimizer: str\n",
    "    evals_per_epoch: float\n",
    "\n",
    "\n",
    "from model.gpt import GPT, GPTConfig\n",
    "\n",
    "@dataclass\n",
    "class AttnConfig:\n",
    "    d_model: int\n",
    "    n_head: int\n",
    "    block_size: int\n",
    "    dropout: float\n",
    "\n",
    "cfg = OmegaConf.load(\"config/hyperparams.yaml\")\n",
    "            # Update cfg with args\n",
    "\n",
    "hparams = OmegaConf.to_container(cfg.hyperparams, resolve=True)\n",
    "modelparams = OmegaConf.to_container(cfg.model_configs[hparams['model_architecture']], resolve=True)\n",
    "attnparams = OmegaConf.to_container(cfg.attn_configs[modelparams['attention_layer']], resolve=True)\n",
    "\n",
    "args = Hyperparameters(**hparams)\n",
    "\n",
    "attn = AttnConfig(\n",
    "    d_model=modelparams['d_model'],\n",
    "    n_head=attnparams['n_head'],\n",
    "    block_size=args.context_length,\n",
    "    dropout=modelparams['dropout']\n",
    ")\n",
    "\n",
    "cfg = GPTConfig(\n",
    "    vocab_size=args.vocab_size,\n",
    "    block_size=args.context_length,\n",
    "    attn_config = attn,\n",
    "    activation_function = 'gelu',\n",
    "    **modelparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f943360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-plasma-1</strong> at: <a href='https://wandb.ai/arc_agi/mainrun/runs/4mti1j53' target=\"_blank\">https://wandb.ai/arc_agi/mainrun/runs/4mti1j53</a><br> View project at: <a href='https://wandb.ai/arc_agi/mainrun' target=\"_blank\">https://wandb.ai/arc_agi/mainrun</a><br>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250905_134930-4mti1j53\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\teeds\\Desktop\\projects\\mainrun\\mainrun\\wandb\\run-20250905_135547-rxpm6y30</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arc_agi/mainrun/runs/rxpm6y30' target=\"_blank\">swift-waterfall-2</a></strong> to <a href='https://wandb.ai/arc_agi/mainrun' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arc_agi/mainrun' target=\"_blank\">https://wandb.ai/arc_agi/mainrun</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arc_agi/mainrun/runs/rxpm6y30' target=\"_blank\">https://wandb.ai/arc_agi/mainrun/runs/rxpm6y30</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "<class 'wandb.sdk.wandb_config.Config'> object has no attribute 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\sdk\\wandb_config.py:165\u001b[0m, in \u001b[0;36mConfig.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ke:\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\sdk\\wandb_config.py:130\u001b[0m, in \u001b[0;36mConfig.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lr'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit()\n\u001b[0;32m      5\u001b[0m cfg \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cfg\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse.attn_type\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\sdk\\wandb_config.py:167\u001b[0m, in \u001b[0;36mConfig.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ke:\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mke\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: <class 'wandb.sdk.wandb_config.Config'> object has no attribute 'lr'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "\n",
    "cfg = wandb.config\n",
    "\n",
    "print(\"Learning rate:\", cfg.lr)\n",
    "print(\"Batch size:\", cfg.batch_size)\n",
    "print(\"Attention type:\", cfg[\"sparse.attn_type\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03564130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sparseK(u, k):\n",
    "    B, T = u.shape\n",
    "    z_sorted, _ = torch.sort(u, dim=1, descending=True)  # (B, T)\n",
    "    z_cumsum = torch.cumsum(z_sorted, dim=1)  # (B, T)\n",
    "\n",
    "    # candidates: z and z-1\n",
    "    beta_candidates = torch.cat([z_sorted, z_sorted - 1], dim=1)  # (B, 2*T)\n",
    "    beta_sorted, _ = torch.sort(beta_candidates, dim=1, descending=True)\n",
    "\n",
    "    tau = torch.zeros(B, device=u.device)\n",
    "    done = torch.zeros(B, dtype=torch.bool, device=u.device)\n",
    "    p = None\n",
    "\n",
    "    batch_idx = torch.arange(B, device=u.device)\n",
    "\n",
    "    print(\"Input u:\\n\", u)\n",
    "    print(\"Sorted z:\\n\", z_sorted)\n",
    "    print(\"Cumsum z:\\n\", z_cumsum)\n",
    "    print(\"Beta candidates shape:\", beta_candidates.shape)\n",
    "    print(\"Beta sorted shape:\", beta_sorted.shape)\n",
    "\n",
    "    for i in range(beta_sorted.shape[1]):\n",
    "        beta = beta_sorted[:, i][:, None]  # (B, 1)\n",
    "\n",
    "        # indices\n",
    "        u_idx = (z_sorted >= (beta + 1)).int().sum(dim=1) - 1\n",
    "        w_idx = (z_sorted > beta).int().sum(dim=1) - 1\n",
    "\n",
    "        denom = (w_idx - u_idx).float()\n",
    "        denom = torch.where(denom == 0, torch.ones_like(denom), denom)\n",
    "\n",
    "        tau_candidate = (\n",
    "            (z_cumsum[batch_idx, w_idx] - z_cumsum[batch_idx, u_idx])\n",
    "            + u_idx - k\n",
    "        ) / denom\n",
    "\n",
    "        # candidate projection\n",
    "        p_candidate = torch.clamp(z_sorted - tau_candidate[:, None], 0, 1)\n",
    "        sum_p = p_candidate.sum(dim=1)\n",
    "\n",
    "        cond_sum = (sum_p - k).abs() < 1e-6\n",
    "        cond = (z_sorted[batch_idx, w_idx] > tau_candidate) & \\\n",
    "               (z_sorted[batch_idx, u_idx] >= tau_candidate + 1) & \\\n",
    "               cond_sum\n",
    "\n",
    "        print(f\"\\n--- Iter {i} ---\")\n",
    "        print(\"beta:\", beta.squeeze())\n",
    "        print(\"u_idx:\", u_idx)\n",
    "        print(\"w_idx:\", w_idx)\n",
    "        print(\"tau_candidate:\", tau_candidate)\n",
    "        print(\"sum_p:\", sum_p)\n",
    "        print(\"cond:\", cond)\n",
    "\n",
    "        tau[~done & cond] = tau_candidate[~done & cond]\n",
    "        done = done | cond\n",
    "\n",
    "        if done.all():\n",
    "            p = p_candidate\n",
    "            print(\">>> Found valid tau at iter\", i)\n",
    "            break\n",
    "\n",
    "    if p is None:\n",
    "        print(\"No candidate satisfied condition → fallback to hard top-k\")\n",
    "        p = torch.clamp(z_sorted - tau[:, None], 0, 1)\n",
    "        for b in range(B):\n",
    "            if not torch.isclose(p[b].sum(), torch.tensor(float(k), device=u.device), atol=1e-6):\n",
    "                topk_idx = torch.topk(z_sorted[b], k).indices\n",
    "                p[b] = torch.zeros_like(z_sorted[b])\n",
    "                p[b, topk_idx] = 1.0\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db92b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'program': 'train.py', 'method': 'random', 'metric': {'name': 'val_loss', 'goal': 'minimize'}, 'parameters': {'sparse.n_head': {'values': [4, 8, 16]}, 'sparse.num_verts': {'values': [4, 8, 16]}, 'sparse.sparseblocksize': {'values': [32, 64, 128]}, 'sparse.vertsize': {'values': [64, 128, 256]}, 'sparse.n_bctx': {'values': [1, 2, 4]}, 'sparse.intermediate_dim': {'values': [0, 64, 128]}}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open('config/sweep_gpt_sparse.yaml') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ba0cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf62d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dotted_keys(base_dict, update_dict, target_path=None):\n",
    "    \"\"\"\n",
    "    Merge keys with dots into nested dicts.\n",
    "    If target_path is given, merge inside that nested dict.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    merged = copy.deepcopy(base_dict)\n",
    "    \n",
    "    # if target_path is provided, get the nested dict\n",
    "    if target_path:\n",
    "        d = merged\n",
    "        for k in target_path:\n",
    "            d = d.setdefault(k, {})\n",
    "    else:\n",
    "        d = merged\n",
    "    \n",
    "    for key, value in update_dict.items():\n",
    "        parts = key.split(\".\")\n",
    "        curr = d\n",
    "        for p in parts[:-1]:\n",
    "            curr = curr.setdefault(p, {})\n",
    "        curr[parts[-1]] = value\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a0a529",
   "metadata": {},
   "outputs": [
    {
     "ename": "CommError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\apis\\normalize.py:25\u001b[0m, in \u001b[0;36mnormalize_exceptions.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\sdk\\internal\\internal_api.py:3341\u001b[0m, in \u001b[0;36mApi.upsert_sweep\u001b[1;34m(self, config, controller, launch_scheduler, scheduler, obj_id, project, entity, state, prior_runs, display_name, template_variable_values)\u001b[0m\n\u001b[0;32m   3339\u001b[0m mutations \u001b[38;5;241m=\u001b[39m [mutation_5, mutation_4, mutation_3, mutation_2, mutation_1]\n\u001b[1;32m-> 3341\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_config_and_fill_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3343\u001b[0m \u001b[38;5;66;03m# Silly, but attr-dicts like EasyDicts don't serialize correctly to yaml.\u001b[39;00m\n\u001b[0;32m   3344\u001b[0m \u001b[38;5;66;03m# This sanitizes them with a round trip pass through json to get a regular dict.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\sdk\\internal\\internal_api.py:3201\u001b[0m, in \u001b[0;36mApi._validate_config_and_fill_distribution\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m   3198\u001b[0m \u001b[38;5;66;03m# explicitly cast to dict in case config was passed as a sweepconfig\u001b[39;00m\n\u001b[0;32m   3199\u001b[0m \u001b[38;5;66;03m# sweepconfig does not serialize cleanly to yaml and breaks graphql,\u001b[39;00m\n\u001b[0;32m   3200\u001b[0m \u001b[38;5;66;03m# but it is a subclass of dict, so this conversion is clean\u001b[39;00m\n\u001b[1;32m-> 3201\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[0;32m   3204\u001b[0m     \u001b[38;5;66;03m# still shows an anaconda warning, but doesn't error\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 1; 2 is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sweep_id \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msweep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig/sweep_gpt_sparse.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-from-scratch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marc_agi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\sdk\\wandb_sweep.py:87\u001b[0m, in \u001b[0;36msweep\u001b[1;34m(sweep, entity, project, prior_runs)\u001b[0m\n\u001b[0;32m     85\u001b[0m     wandb_login\u001b[38;5;241m.\u001b[39m_login(_silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     86\u001b[0m api \u001b[38;5;241m=\u001b[39m InternalApi()\n\u001b[1;32m---> 87\u001b[0m sweep_id, warnings \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert_sweep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msweep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior_runs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m handle_sweep_config_violations(warnings)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate sweep with ID:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sweep_id)  \u001b[38;5;66;03m# noqa: T201\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\apis\\internal.py:137\u001b[0m, in \u001b[0;36mApi.upsert_sweep\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupsert_sweep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mupsert_sweep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\apis\\normalize.py:79\u001b[0m, in \u001b[0;36mnormalize_exceptions.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CommError(message, err)\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\apis\\normalize.py:25\u001b[0m, in \u001b[0;36mnormalize_exceptions.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhoa, you found a bug.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m     28\u001b[0m     errors \u001b[38;5;241m=\u001b[39m parse_backend_error_messages(error\u001b[38;5;241m.\u001b[39mresponse)\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\sdk\\internal\\internal_api.py:3341\u001b[0m, in \u001b[0;36mApi.upsert_sweep\u001b[1;34m(self, config, controller, launch_scheduler, scheduler, obj_id, project, entity, state, prior_runs, display_name, template_variable_values)\u001b[0m\n\u001b[0;32m   3338\u001b[0m \u001b[38;5;66;03m# TODO(dag): replace this with a query for protocol versioning\u001b[39;00m\n\u001b[0;32m   3339\u001b[0m mutations \u001b[38;5;241m=\u001b[39m [mutation_5, mutation_4, mutation_3, mutation_2, mutation_1]\n\u001b[1;32m-> 3341\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_config_and_fill_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3343\u001b[0m \u001b[38;5;66;03m# Silly, but attr-dicts like EasyDicts don't serialize correctly to yaml.\u001b[39;00m\n\u001b[0;32m   3344\u001b[0m \u001b[38;5;66;03m# This sanitizes them with a round trip pass through json to get a regular dict.\u001b[39;00m\n\u001b[0;32m   3345\u001b[0m config_str \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mdump(\n\u001b[0;32m   3346\u001b[0m     json\u001b[38;5;241m.\u001b[39mloads(json\u001b[38;5;241m.\u001b[39mdumps(config)), Dumper\u001b[38;5;241m=\u001b[39mutil\u001b[38;5;241m.\u001b[39mNonOctalStringDumper\n\u001b[0;32m   3347\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\sdk\\internal\\internal_api.py:3201\u001b[0m, in \u001b[0;36mApi._validate_config_and_fill_distribution\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m   3196\u001b[0m config \u001b[38;5;241m=\u001b[39m deepcopy(config)\n\u001b[0;32m   3198\u001b[0m \u001b[38;5;66;03m# explicitly cast to dict in case config was passed as a sweepconfig\u001b[39;00m\n\u001b[0;32m   3199\u001b[0m \u001b[38;5;66;03m# sweepconfig does not serialize cleanly to yaml and breaks graphql,\u001b[39;00m\n\u001b[0;32m   3200\u001b[0m \u001b[38;5;66;03m# but it is a subclass of dict, so this conversion is clean\u001b[39;00m\n\u001b[1;32m-> 3201\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[0;32m   3204\u001b[0m     \u001b[38;5;66;03m# still shows an anaconda warning, but doesn't error\u001b[39;00m\n\u001b[0;32m   3205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[1;31mCommError\u001b[0m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep('config/sweep_gpt_sparse.yaml', project=\"gpt-from-scratch\", entity=\"arc_agi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c3464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ced1108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('config\\sweep_gpt_sparse.yaml')\n",
    "# Convert to a plain dictionary\n",
    "cfg_dict = OmegaConf.to_container(cfg, resolve=True)\n",
    "\n",
    "\n",
    "orig_cfg = OmegaConf.load('config\\hparams_gpt_sparse.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52d152c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = 'config\\sweep_gpt_sparse.yaml'\n",
    "cfg = 'config\\sweep_gpt_sparse.yaml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa99ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_train():\n",
    "    orig_cfg = OmegaConf.load('config\\hparams_gpt_sparse.yaml')  # defaults\n",
    "    with wandb.init() as run:\n",
    "        print(run)\n",
    "        sweep_cfg = OmegaConf.create({\"hyperparams\": dict(run.config)})\n",
    "        cfg = OmegaConf.merge(orig_cfg, sweep_cfg)\n",
    "        print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef8bc7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'program': 'train.py', 'method': 'random', 'metric': {'name': 'val_loss', 'goal': 'minimize'}, 'parameters': {'sparse.n_head': {'values': [4, 8, 16]}, 'sparse.num_verts': {'values': [4, 8, 16]}, 'sparse.sparseblocksize': {'values': [32, 64, 128]}, 'sparse.vertsize': {'values': [64, 128, 256]}, 'sparse.n_bctx': {'values': [1, 2, 4]}, 'sparse.intermediate_dim': {'values': [0, 64, 128]}}}\n",
      "Create sweep with ID: t35wi9ov\n",
      "Sweep URL: https://wandb.ai/arc_agi/gpt-from-scratch/sweeps/t35wi9ov\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hrcytqn4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.intermediate_dim: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.n_bctx: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.n_head: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.num_verts: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.sparseblocksize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.vertsize: 128\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\teeds\\AppData\\Local\\Temp\\ipykernel_229868\\385059661.py\", line 2, in sweep_train\n",
      "    orig_cfg = OmegaConf.load(args.orig_yaml)  # defaults\n",
      "NameError: name 'args' is not defined\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run hrcytqn4 errored: name 'args' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4gw97p56 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.intermediate_dim: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.n_bctx: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.n_head: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.num_verts: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.sparseblocksize: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.vertsize: 256\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\teeds\\AppData\\Local\\Temp\\ipykernel_229868\\385059661.py\", line 2, in sweep_train\n",
      "    orig_cfg = OmegaConf.load(args.orig_yaml)  # defaults\n",
      "NameError: name 'args' is not defined\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4gw97p56 errored: name 'args' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iz3xvov4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.intermediate_dim: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.n_bctx: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.n_head: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.num_verts: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.sparseblocksize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsparse.vertsize: 128\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\teeds\\AppData\\Local\\Temp\\ipykernel_229868\\385059661.py\", line 2, in sweep_train\n",
      "    orig_cfg = OmegaConf.load(args.orig_yaml)  # defaults\n",
      "NameError: name 'args' is not defined\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run iz3xvov4 errored: name 'args' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load(sweep_config)\n",
    "# Convert to a plain dictionary\n",
    "cfg_dict = OmegaConf.to_container(cfg, resolve=True)\n",
    "print(cfg_dict)\n",
    "sweep_id = wandb.sweep(cfg_dict, project=\"gpt-from-scratch\", entity=\"arc_agi\")\n",
    "wandb.agent(sweep_id, function=sweep_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e6a8712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse.n_head {'values': 4}\n",
      "sparse.num_verts {'values': 4}\n",
      "sparse.sparseblocksize {'values': 32}\n",
      "sparse.vertsize {'values': 64}\n",
      "sparse.n_bctx {'values': 1}\n",
      "sparse.intermediate_dim {'values': 0}\n"
     ]
    }
   ],
   "source": [
    "for key, value in sweep_cfg['parameters'].items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e8c299e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hyperparams': {'seed': 1337, 'epochs': 7, 'val_frac': 0.1, 'num_titles': 100000, 'vocab_size': 16000, 'context_length': 256, 'model_architecture': 'gpt', 'log_file': './logs/mainrun.log', 'batch_size': 128, 'lr': 0.007, 'weight_decay': 0.0, 'scheduler': 'cosine', 'optimizer': 'adagrad', 'evals_per_epoch': 3}, 'model_configs': {'gpt': {'d_model': 256, 'hidden_layer': 256, 'n_layer': 6, 'dropout': 0.1, 'init_method': 'xavier', 'attention_layer': 'sparse'}, 'unet_gpt': {'d_model': 512, 'hidden_layer': 128, 'n_layer': 6, 'dropout': 0.1, 'init_method': 'xavier', 'attention_layer': 'sparse', 'bottleneck_sizes': [512, 256, 256, 128, 128, 256]}}, 'attn_configs': {'causal': {'n_head': 8, 'intermediate_dim': 0}, 'sparse': {'attn_type': 'fixed', 'n_head': 8, 'num_verts': 8, 'local_attn_ctx': 32, 'sparseblocksize': 64, 'vertsize': 128, 'n_bctx': 2, 'intermediate_dim': 0}}, 'program': 'train.py', 'method': 'random', 'metric': {'name': 'val_loss', 'goal': 'minimize'}, 'parameters': {'sparse.n_head': {'values': 4}, 'sparse.num_verts': {'values': 4}, 'sparse.sparseblocksize': {'values': 32}, 'sparse.vertsize': {'values': 64}, 'sparse.n_bctx': {'values': 1}, 'sparse.intermediate_dim': {'values': 0}}}\n"
     ]
    }
   ],
   "source": [
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d3e33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dotted_keys(base_dict, update_dict, target_path=None):\n",
    "    \"\"\"\n",
    "    Merge keys with dots into nested dicts.\n",
    "    If target_path is given, merge inside that nested dict.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    merged = copy.deepcopy(base_dict)\n",
    "    \n",
    "    # if target_path is provided, get the nested dict\n",
    "    if target_path:\n",
    "        d = merged\n",
    "        for k in target_path:\n",
    "            d = d.setdefault(k, {})\n",
    "    else:\n",
    "        d = merged\n",
    "    \n",
    "    for key, value in update_dict.items():\n",
    "        parts = key.split(\".\")\n",
    "        curr = d\n",
    "        for p in parts[:-1]:\n",
    "            curr = curr.setdefault(p, {})\n",
    "        curr[parts[-1]] = value\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2844d69f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigKeyError",
     "evalue": "Missing key hyperparams\n    full_key: hyperparams\n    object_type=dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConfigKeyError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Suppose cfg_dict comes from OmegaConf.to_container()\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sweep_params \u001b[38;5;241m=\u001b[39m {k: v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m cfg_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()}  \u001b[38;5;66;03m# pick first value for example\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m hyperparams \u001b[38;5;241m=\u001b[39m \u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyperparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m attn_configs \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_configs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m hyperparams \u001b[38;5;241m=\u001b[39m merge_dotted_keys(base_dict\u001b[38;5;241m=\u001b[39mhyperparams, update_dict\u001b[38;5;241m=\u001b[39msweep_params, target_path\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_configs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\omegaconf\\dictconfig.py:375\u001b[0m, in \u001b[0;36mDictConfig.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(\n\u001b[0;32m    372\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me, type_override\u001b[38;5;241m=\u001b[39mConfigKeyError\n\u001b[0;32m    373\u001b[0m     )\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_and_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\omegaconf\\base.py:231\u001b[0m, in \u001b[0;36mNode._format_and_raise\u001b[1;34m(self, key, value, cause, msg, type_override)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_format_and_raise\u001b[39m(\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    225\u001b[0m     key: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     type_override: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     \u001b[43mformat_and_raise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcause\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtype_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\omegaconf\\_utils.py:899\u001b[0m, in \u001b[0;36mformat_and_raise\u001b[1;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[0;32m    896\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type \u001b[38;5;241m=\u001b[39m ref_type\n\u001b[0;32m    897\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type_str \u001b[38;5;241m=\u001b[39m ref_type_str\n\u001b[1;32m--> 899\u001b[0m \u001b[43m_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\omegaconf\\_utils.py:797\u001b[0m, in \u001b[0;36m_raise\u001b[1;34m(ex, cause)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     ex\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 797\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\omegaconf\\dictconfig.py:369\u001b[0m, in \u001b[0;36mDictConfig.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03mAllow map style access\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m:param key:\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_DEFAULT_MARKER_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(\n\u001b[0;32m    372\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me, type_override\u001b[38;5;241m=\u001b[39mConfigKeyError\n\u001b[0;32m    373\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\omegaconf\\dictconfig.py:442\u001b[0m, in \u001b[0;36mDictConfig._get_impl\u001b[1;34m(self, key, default_value, validate_key)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_impl\u001b[39m(\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m, key: DictKeyType, default_value: Any, validate_key: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    440\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m         node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_child\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthrow_on_missing_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_key\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (ConfigAttributeError, ConfigKeyError):\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _DEFAULT_MARKER_:\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\omegaconf\\basecontainer.py:73\u001b[0m, in \u001b[0;36mBaseContainer._get_child\u001b[1;34m(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_child\u001b[39m(\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     66\u001b[0m     key: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     throw_on_missing_key: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Optional[Node], List[Optional[Node]]]:\n\u001b[0;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Like _get_node, passing through to the nearest concrete Node.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrow_on_missing_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrow_on_missing_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrow_on_missing_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrow_on_missing_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, UnionNode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_special(child):\n\u001b[0;32m     81\u001b[0m         value \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39m_value()\n",
      "File \u001b[1;32mc:\\Users\\teeds\\miniconda3\\envs\\llm_train\\lib\\site-packages\\omegaconf\\dictconfig.py:480\u001b[0m, in \u001b[0;36mDictConfig._get_node\u001b[1;34m(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m throw_on_missing_key:\n\u001b[1;32m--> 480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigKeyError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m throw_on_missing_value \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39m_is_missing():\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingMandatoryValue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing mandatory value: $KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mConfigKeyError\u001b[0m: Missing key hyperparams\n    full_key: hyperparams\n    object_type=dict"
     ]
    }
   ],
   "source": [
    "# Suppose cfg_dict comes from OmegaConf.to_container()\n",
    "sweep_params = {k: v['values'][0] for k, v in cfg_dict['parameters'].items()}  # pick first value for example\n",
    "hyperparams = cfg['hyperparams']\n",
    "attn_configs = cfg['attn_configs']\n",
    "\n",
    "hyperparams = merge_dotted_keys(base_dict=hyperparams, update_dict=sweep_params, target_path=['attn_configs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff47d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

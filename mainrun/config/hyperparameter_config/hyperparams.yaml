hyperparams:
  #############################################
  # FIXED hyperparameters
  #############################################
  seed: 1337
  epochs: 7
  val_frac: 0.10
  num_titles: 100000
  vocab_size: 16000 # Vocabulary size of the tokenizer
  context_length: 256
  ##############################################
  # CHANGEABLE hyperparameters
  ##############################################
  model_architecture: "gpt" # gpt, bottleneck_gpt, unet_gpt
  log_file: "./logs/mainrun.log"
  ###############################################
  # Training hyperparameters
  ###############################################
  batch_size: 32
  lr: 0.005
  weight_decay: 0.1
  scheduler: "cosine" # none, linear, cosine
  optimizer: "adamw" 
  evals_per_epoch: 3
  amp_bool: True



model_configs:
  gpt:
    d_model: 128
    hidden_layer: 128
    n_layer: 6
    dropout: 0.1
    init_method: 'xavier'
    attention_layer: 'sparse'
  unet_gpt:
    d_model: 512
    hidden_layer: 128
    n_layer: 6
    dropout: 0.1
    init_method: 'xavier'
    attention_layer: 'causal'
    bottleneck_sizes: [512, 256, 256, 128, 128, 256]

attn_configs:
  causal:
    n_head: 8
    intermediate_dim : 0 # if 0 , normal , elif > 0 , bottleneck
  sparse:
    attn_type: 'fixed'  # 'all', 'fixed', 'local'. 'strided'
    n_head: 8
    num_verts: 8   #
    local_attn_ctx: 32
    sparseblocksize: 64
    vertsize: 128
    n_bctx: 2
    intermediate_dim : 0 # if 0 , normal , elif > 0 , bottleneck


      #   UNET d_model: 512
  #   n_head: 8
  #   n_layer: 6
  #   dropout: 0.1
  #   bottleneck_size: 256
  # unet_gpt:
hyperparams:
  #############################################
  # FIXED hyperparameters
  #############################################
  seed: 1337
  epochs: 7
  val_frac: 0.10
  num_titles: 100000
  vocab_size: 16000 # Vocabulary size of the tokenizer
  context_length: 256
  ##############################################
  # CHANGEABLE hyperparameters
  ##############################################
  model_architecture: "bottleneck_gpt" # gpt, bottleneck_gpt, unet_gpt
  log_file: "./logs/mainrun.log"
  ###############################################
  # MODEL hyperparameters (also in model.yaml)
  ###############################################
  batch_size: 64
  n_layer: 6
  n_head: 8
  d_model: 512 # Dimension of the model
  dropout: 0.1
  bottleneck_size: 256  # Set to 0 for no bottleneck, >0 for bottleneck size
  attention_layer: 'sparse'
  ###############################################
  # Training hyperparameters
  ###############################################
  lr: 0.006
  weight_decay: 0.0
  scheduler: "cosine" # none, linear, cosine
  optimizer: "sgd" 
  evals_per_epoch: 3




model_configs:
  gpt:
    d_model: 512
    n_head: 8
    n_layer: 6
    dropout: 0.1
  bottleneck_gpt:
    d_model: 512
    n_head: 8
    n_layer: 6
    dropout: 0.1
    bottleneck_sizes: [512, 256, 256, 128, 128, 256]



      #   UNET d_model: 512
  #   n_head: 8
  #   n_layer: 6
  #   dropout: 0.1
  #   bottleneck_size: 256
  # unet_gpt:
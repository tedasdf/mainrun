{'hyperparams': {'seed': 1337, 'epochs': 7, 'val_frac': 0.1, 'num_titles': 100000, 'vocab_size': 16000, 'context_length': 256, 'model_architecture': 'gpt', 'log_file': './logs/mainrun.log', 'batch_size': 128, 'lr': 0.007, 'weight_decay': 0.0, 'scheduler': 'cosine', 'optimizer': 'adagrad', 'evals_per_epoch': 3, 'sparse.intermediate_dim': 64, 'sparse.n_bctx': 2, 'sparse.n_head': 8, 'sparse.num_verts': 4, 'sparse.sparseblocksize': 128, 'sparse.vertsize': 128}, 'model_configs': {'gpt': {'d_model': 256, 'hidden_layer': 256, 'n_layer': 6, 'dropout': 0.1, 'init_method': 'xavier', 'attention_layer': 'sparse'}, 'unet_gpt': {'d_model': 512, 'hidden_layer': 128, 'n_layer': 6, 'dropout': 0.1, 'init_method': 'xavier', 'attention_layer': 'sparse', 'bottleneck_sizes': [512, 256, 256, 128, 128, 256]}}, 'attn_configs': {'causal': {'n_head': 8, 'intermediate_dim': 0}, 'sparse': {'attn_type': 'fixed', 'n_head': 8, 'num_verts': 8, 'local_attn_ctx': 32, 'sparseblocksize': 64, 'vertsize': 128, 'n_bctx': 2, 'intermediate_dim': 0}}}